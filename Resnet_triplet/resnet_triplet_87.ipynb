{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.initializers import HeUniform, GlorotNormal\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
    "from keras.layers import LeakyReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from keras.regularizers import L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from itertools import combinations, product\n",
    "from random import sample, shuffle, seed\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.math import reduce_sum, square, reduce_mean, maximum, sqrt\n",
    "from tensorflow import random\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from keras.layers import Conv2D, ZeroPadding2D, Activation, Input, concatenate, Dropout, BatchNormalization\n",
    "from keras import layers\n",
    "from tensorflow.keras import optimizers\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.models import Model, Sequential\n",
    "from keras.applications import resnet\n",
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.layers.core import Lambda, Flatten, Dense\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.applications import resnet\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "from math import ceil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "random.set_seed(42)\n",
    "\n",
    "img_h, img_w = 155, 220\n",
    "\n",
    "dirs_list=os.listdir(\"E:\\Academics\\Project\\BHSig260-Hindi\\BHSig260-Hindi\")\n",
    "\n",
    "path=\"E:\\Academics\\Project\\BHSig260-Hindi\\BHSig260-Hindi\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_dataset(from_dir, to_dir):\n",
    "    org_sign=[]\n",
    "    forg_sign=[]\n",
    "    for directory in dirs_list[from_dir:to_dir]:\n",
    "        images = os.listdir(path+directory)\n",
    "        images.sort()\n",
    "        images = [directory+'\\\\'+x for x in images]\n",
    "        forg_sign.append(images[:30])\n",
    "        org_sign.append(images[30:])\n",
    "\n",
    "    data=[]\n",
    "    for i in range(len(org_sign)):\n",
    "        j=0\n",
    "        for signs in list(map(list,combinations(org_sign[i],2)))[:170]:\n",
    "            data.append((*signs, forg_sign[i][j%30]))\n",
    "            j+=1\n",
    "    del org_sign, forg_sign, images\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_train=get_dataset(0, 110)\n",
    "X_valid=get_dataset(110, 135)\n",
    "X_test=get_dataset(135, 160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(filename=\"newfile.log\",format='%(asctime)s %(message)s',filemode='w')\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class SignatureSequence(Sequence):\n",
    "    \n",
    "    def __init__(self, X, batch_size, dim):\n",
    "        self.X=X\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return ceil(len(self.X) / self.batch_size)-1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_X = self.X[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        \n",
    "        part_1 = np.empty((self.batch_size, *self.dim,3))\n",
    "        part_2 = np.empty((self.batch_size, *self.dim,3))\n",
    "        part_3 = np.empty((self.batch_size, *self.dim,3))\n",
    "        \n",
    "        for i in range(len(batch_X)):\n",
    "            part_1[i,]=self.image_preprocessing(batch_X[i][0])\n",
    "            part_2[i,]=self.image_preprocessing(batch_X[i][1])\n",
    "            part_3[i,]=self.image_preprocessing(batch_X[i][2])\n",
    "            batch_X_pro=(part_1 ,part_2, part_3)\n",
    "        return batch_X_pro\n",
    "\n",
    "    def image_preprocessing(self, signature):\n",
    "        signature = cv2.imread(path+signature)\n",
    "        resized_signature = cv2.resize(signature,(220,155))\n",
    "        blur = cv2.GaussianBlur(resized_signature,(3,3),-125)\n",
    "        gray_signature=cv2.cvtColor(blur, cv2.COLOR_BGR2GRAY)\n",
    "        ret,thr_img = cv2.threshold(gray_signature, 0, 255, cv2.THRESH_OTSU)\n",
    "        normalized_signature=1-(thr_img/255)\n",
    "        rgb_batch = np.repeat(normalized_signature[..., np.newaxis], 3, -1)\n",
    "        #signature_expanded = normalized_signature[:, :, np.newaxis]\n",
    "        return np.array(rgb_batch)\n",
    "    def on_epoch_end(self):        \n",
    "        np.random.shuffle(self.X)\n",
    "        logger.debug(\"Called\"+str(self.X[0]))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def euclidean_distance(vectors):\n",
    "    x, y = vectors\n",
    "    return K.sqrt(K.sum(K.square(x - y), axis=1, keepdims=True))\n",
    "\n",
    "def output_shape(shapes):\n",
    "    shape1, shape2 = shapes\n",
    "    return (shape1[0], 1)\n",
    "\n",
    "def contrastive_loss(y_true, y_pred):\n",
    "    margin = 1\n",
    "    return K.mean(y_true * K.square(y_pred) + (1 - y_true) * K.square(K.maximum(margin - y_pred, 0)))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "input_shape=(155,220,3)\n",
    "base_cnn = resnet.ResNet50(\n",
    "    weights=\"imagenet\", input_shape=input_shape, include_top=False\n",
    ")\n",
    "\n",
    "flatten = GlobalAveragePooling2D()(base_cnn.output)\n",
    "dense1 = layers.Dense(512, activation=\"relu\")(flatten)\n",
    "dr1=Dropout(0.3)(dense1)\n",
    "b1 = layers.BatchNormalization()(dr1)\n",
    "\n",
    "dense2 = layers.Dense(256, activation=\"relu\")(b1)\n",
    "dr2=Dropout(0.3)(dense2)\n",
    "b2 = layers.BatchNormalization()(dr2)\n",
    "output = layers.Dense(256)(b2)\n",
    "\n",
    "embedding = Model(base_cnn.input, output, name=\"Embedding\")\n",
    "\n",
    "trainable = False\n",
    "for layer in base_cnn.layers:\n",
    "    if layer.name == \"conv5_block1_out\":\n",
    "        trainable = True\n",
    "    layer.trainable = trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape=(155,220,3)\n",
    "base_cnn = resnet.ResNet50(\n",
    "    weights=\"imagenet\", input_shape=input_shape, include_top=False\n",
    ")\n",
    "\n",
    "flatten = GlobalAveragePooling2D()(base_cnn.output)\n",
    "dense1 = layers.Dense(512, activation=\"relu\", kernel_regularizer=L2(0.001))(flatten)\n",
    "dense1 = Dropout(0.2)(dense1)\n",
    "dense1 = layers.BatchNormalization()(dense1)\n",
    "\n",
    "dense2 = layers.Dense(256, activation=\"relu\", kernel_regularizer=L2(0.001))(dense1)\n",
    "dense2 = Dropout(0.2)(dense2)\n",
    "dense2 = layers.BatchNormalization()(dense2)\n",
    "\n",
    "output = layers.Dense(256)(dense2)\n",
    "\n",
    "embedding = Model(base_cnn.input, output, name=\"Embedding\")\n",
    "\n",
    "trainable = False\n",
    "for layer in base_cnn.layers:\n",
    "    if layer.name == \"conv5_block1_out\":\n",
    "        trainable = True\n",
    "    layer.trainable = trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistanceLayer(layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def call(self, anchor, positive, negative):\n",
    "        \n",
    "        sum_square1 = tf.math.reduce_sum(tf.math.square(anchor - positive), axis=1, keepdims=True)\n",
    "        ap_distance = tf.math.sqrt(tf.math.maximum(sum_square1, tf.keras.backend.epsilon()))\n",
    "        \n",
    "        sum_square2 = tf.math.reduce_sum(tf.math.square(anchor - negative), axis=1, keepdims=True)\n",
    "        an_distance = tf.math.sqrt(tf.math.maximum(sum_square2, tf.keras.backend.epsilon()))\n",
    "        \n",
    "        \n",
    "        '''ap_distance = tf.reduce_sum(tf.square(anchor - positive), -1)\n",
    "        an_distance = tf.reduce_sum(tf.square(anchor - negative), -1)'''\n",
    "        \n",
    "        return (ap_distance, an_distance)\n",
    "\n",
    "\n",
    "anchor_input = layers.Input(name=\"anchor\", shape=input_shape)\n",
    "positive_input = layers.Input(name=\"positive\", shape=input_shape)\n",
    "negative_input = layers.Input(name=\"negative\", shape=input_shape)\n",
    "\n",
    "distances = DistanceLayer()(\n",
    "    embedding(anchor_input),\n",
    "    embedding(positive_input),\n",
    "    embedding(negative_input),\n",
    ")\n",
    "\n",
    "siamese_network = Model(\n",
    "    inputs=[anchor_input, positive_input, negative_input], outputs=distances\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseModel(Model):\n",
    "    def __init__(self, siamese_network, margin=0.5):\n",
    "        super(SiameseModel, self).__init__()\n",
    "        self.siamese_network = siamese_network\n",
    "        self.margin = margin\n",
    "        self.loss_tracker = metrics.Mean(name=\"loss\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.siamese_network(inputs)\n",
    "\n",
    "    def train_step(self, data):\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = self._compute_loss(data)\n",
    "        gradients = tape.gradient(loss, self.siamese_network.trainable_weights)\n",
    "        self.optimizer.apply_gradients(\n",
    "            zip(gradients, self.siamese_network.trainable_weights)\n",
    "        )\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        return {\"loss\": self.loss_tracker.result()}\n",
    "\n",
    "    def test_step(self, data):\n",
    "        loss = self._compute_loss(data)\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        return {\"loss\": self.loss_tracker.result()}\n",
    "\n",
    "    def _compute_loss(self, data):\n",
    "        ap_distance, an_distance = self.siamese_network(data)\n",
    "        square_pred = tf.math.square(an_distance)\n",
    "        margin_square = tf.math.square(tf.math.maximum(self.margin - (an_distance), 0))\n",
    "        loss = tf.math.reduce_mean((1 - ap_distance) * square_pred + (ap_distance) * margin_square)      \n",
    "        \n",
    "        '''loss = ap_distance - an_distance\n",
    "        loss = tf.maximum(loss + self.margin, 0.0)'''\n",
    "        return loss\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.loss_tracker]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese_model = SiameseModel(siamese_network)\n",
    "siamese_model.compile(optimizer=optimizers.Adam(0.0001))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "siamese_model.built=True\n",
    "\n",
    "siamese_model.load_weights('temp.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "nan_terminate=tf.keras.callbacks.TerminateOnNaN()\n",
    "early_stp=EarlyStopping(patience=2, restore_best_weights=True)\n",
    "checkpoint_cb=ModelCheckpoint('shallow_best_model.h5', save_best_only=True, save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "292/292 [==============================] - 518s 2s/step - loss: 0.0014 - val_loss: 2.3987e-04\n",
      "Epoch 2/3\n",
      "292/292 [==============================] - 558s 2s/step - loss: 1.0167e-04 - val_loss: 1.0481e-04\n",
      "Epoch 3/3\n",
      "292/292 [==============================] - 484s 2s/step - loss: 8.0226e-05 - val_loss: 9.7717e-05\n"
     ]
    }
   ],
   "source": [
    "dim=(155,220)\n",
    "batch_size=64\n",
    "train_batch=SignatureSequence(np.array(X_train), batch_size, dim)\n",
    "valid_batch=SignatureSequence(np.array(X_valid),batch_size, dim)\n",
    "history=siamese_model.fit(train_batch, validation_data=valid_batch, epochs=3, steps_per_epoch=len(X_train)//batch_size, validation_steps=len(X_valid)//batch_size, callbacks=[checkpoint_cb, nan_terminate, early_stp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim=(155,220)\n",
    "batch_size=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_batch=SignatureSequence(np.array(X_test),batch_size, dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/66 [==============================] - 81s 1s/step - loss: 8.7804e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8.780429197940975e-05"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "siamese_model.evaluate(test_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/21 [==============================] - 26s 1s/step - loss: 0.0935\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.09352517873048782"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "siamese_model.evaluate(test_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/21 [==============================] - 25s 1s/step - loss: 0.0034\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.003445808310061693"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "siamese_model.evaluate(test_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.02135119\n",
      "0.024533257\n"
     ]
    }
   ],
   "source": [
    "sample = test_batch[0]\n",
    "#visualize(*sample)\n",
    "anchor, positive, negative = sample\n",
    "anchor_embedding, positive_embedding, negative_embedding = (\n",
    "    embedding(anchor),\n",
    "    embedding(positive),\n",
    "    embedding(negative),\n",
    ")\n",
    "\n",
    "cosine_similarity = metrics.CosineSimilarity()\n",
    "\n",
    "positive_similarity = euclidean_distance(anchor_embedding, positive_embedding)\n",
    "#positive_similarity = cosine_similarity(anchor_embedding, positive_embedding)\n",
    "print(positive_similarity.numpy())\n",
    "\n",
    "negative_similarity = euclidean_distance(anchor_embedding, negative_embedding)\n",
    "#negative_similarity = cosine_similarity(anchor_embedding, negative_embedding)\n",
    "print(negative_similarity.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "post=[]\n",
    "neg=[]\n",
    "for anchor, positive, negative in test_batch:\n",
    "    anchor_embedding, positive_embedding, negative_embedding = (embedding(anchor), embedding(positive), embedding(negative))\n",
    "    post.append(euclidean_distance(anchor_embedding, positive_embedding).numpy())\n",
    "    neg.append(euclidean_distance(anchor_embedding, negative_embedding).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.021137705"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.026107714"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=[]\n",
    "for x, y in zip(post, neg):\n",
    "    result.append(str(x-y)[0]=='-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.count(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.count(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8787878787878788"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "58/66"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(x, y ):\n",
    "    sum_square = tf.math.reduce_sum(tf.math.square(x - y), axis=1, keepdims=True)\n",
    "    return tf.math.reduce_sum(tf.math.sqrt(tf.math.maximum(sum_square, tf.keras.backend.epsilon())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(anchor, positive, negative):\n",
    "    \"\"\"Visualize a few triplets from the supplied batches.\"\"\"\n",
    "\n",
    "    def show(ax, image):\n",
    "        ax.imshow(image)\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    fig = plt.figure(figsize=(9, 9))\n",
    "\n",
    "    axs = fig.subplots(3, 3)\n",
    "    for i in range(3):\n",
    "        show(axs[i, 0], anchor[i])\n",
    "        show(axs[i, 1], positive[i])\n",
    "        show(axs[i, 2], negative[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "siamese_model.save_weights('resnet_triplet_best.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.load_weights('temp.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_curve, plot_roc_curve\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def map_arr(arr):\n",
    "    return list(map(int, arr)) \n",
    "def acc_th(th=0.1, step=0.1):\n",
    "    global preds\n",
    "    accs={}\n",
    "    for i in range(0, 40, 1):\n",
    "        th+=step\n",
    "        accs[th]=(accuracy_score(map_arr(np.array(y_test)[:2816]==0), map_arr(preds[:2816]<th)))\n",
    "    best_th=max(accs, key=accs.get)\n",
    "    fpr, tpr, thresholds=roc_curve( map_arr(np.array(y_test)[:2816]==0), map_arr(preds[:2816]<best_th))\n",
    "    plt.plot(fpr)\n",
    "    plt.plot(1-tpr)\n",
    "    plt.show()\n",
    "    return \"Acc: \"+str(max(accs.values())), 'Best Threshold: '+str(best_th)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
